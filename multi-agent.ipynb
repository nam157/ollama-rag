{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b512624-b07c-4476-8b08-f1e3bd9567f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import streamlit as st\n",
    "import faiss\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain.schema.runnable.passthrough import RunnablePassthrough\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "\n",
    "# Add more agents\n",
    "from langchain.agents import create_tool_calling_agent, initialize_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8ce4ab-964d-453b-adab-d65a3403c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_pdf(file_path, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Load a PDF file and split it into chunks.\n",
    "    \"\"\"\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "def create_vector_store_qdrant(doc_chunks, embeddings):\n",
    "    \"\"\"\n",
    "    Create and populate a Qdrant vector store with document chunks.\n",
    "    \"\"\"\n",
    "    # Initialize Qdrant client\n",
    "    qdrant_client = QdrantClient(host=\"10.100.140.54\", port=6333)  # Adjust host and port as needed\n",
    "\n",
    "    # Define the collection name\n",
    "    collection_name = \"document_chunks\"\n",
    "\n",
    "    # Create a collection in Qdrant\n",
    "    qdrant_client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=len(embeddings.embed_query(\"sample text\")), distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "    # Prepare vectors and payloads\n",
    "    vectors = []\n",
    "    payloads = []\n",
    "    for i, chunk in enumerate(doc_chunks):\n",
    "        vector = embeddings.embed_query(chunk.page_content)\n",
    "        vectors.append(vector)\n",
    "        payloads.append({\"id\": i, \"text\": chunk.page_content})\n",
    "\n",
    "    # Upload vectors to Qdrant\n",
    "    qdrant_client.upload_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors=vectors,\n",
    "        payload=payloads\n",
    "    )\n",
    "\n",
    "    return qdrant_client, collection_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1803927f-0ae4-47e5-9edd-5085c60dd451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chatbot_prompt_template():\n",
    "    prompt_text = \"\"\"\n",
    "        You are a helpful assistant answering questions based on the provided context from uploaded documents. Your name is Detnam.\n",
    "\n",
    "        Conversation history (latest messages appear last):\n",
    "        {history}\n",
    "\n",
    "        New question:\n",
    "        {question}\n",
    "\n",
    "        Retrieved context:\n",
    "        {context}\n",
    "        \n",
    "        Thought process:\n",
    "        {agent_scratchpad}\n",
    "\n",
    "        Your response:\n",
    "    \"\"\"\n",
    "    return ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "def format_documents(documents):\n",
    "    \"\"\"\n",
    "    Format a list of documents into a single string with separated contents.\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "817e4ac7-337c-4ee0-9436-37a43b24986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question_with_history_qdrant(uploaded_files, question, history, embeddings, chat_model):\n",
    "    combined_chunks = []\n",
    "\n",
    "    # Process uploaded files and extract chunks\n",
    "    for uploaded_file in uploaded_files:\n",
    "        temp_file_path = f\"temp_{uploaded_file.name}\"\n",
    "        with open(temp_file_path, \"wb\") as f:\n",
    "            f.write(uploaded_file.read())\n",
    "\n",
    "        doc_chunks = load_and_split_pdf(temp_file_path)\n",
    "        combined_chunks.extend(doc_chunks)\n",
    "        os.remove(temp_file_path)\n",
    "\n",
    "    # Create a vector store\n",
    "    qdrant_client, collection_name = create_vector_store_qdrant(combined_chunks, embeddings)\n",
    "\n",
    "    # Retrieve relevant documents\n",
    "    query_vector = embeddings.embed_query(question)\n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=3  # Adjust the number of results as needed\n",
    "    )\n",
    "\n",
    "    # Format the retrieved documents\n",
    "    retrieved_docs = [hit.payload[\"text\"] for hit in search_result]\n",
    "    formatted_context = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "    # Prepare the pipeline\n",
    "    prompt_template = build_chatbot_prompt_template()\n",
    "\n",
    "    # Ensure each part of the pipeline is correctly set up\n",
    "    rag_pipeline = (\n",
    "        RunnablePassthrough()\n",
    "        | {\n",
    "            \"context\": lambda x: formatted_context,  # Use a lambda to pass the context\n",
    "            \"question\": lambda x: question,          # Use a lambda to pass the question\n",
    "            \"history\": lambda x: history,            # Use a lambda to pass the history\n",
    "            \"agent_scratchpad\": lambda x: \"\"\n",
    "        }\n",
    "        | prompt_template\n",
    "        | chat_model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    input_dict = {\n",
    "        \"context\": formatted_context,\n",
    "        \"question\": question,\n",
    "        \"history\": history,\n",
    "        \"agent_scratchpad\": \"\",\n",
    "    }\n",
    "    logging.debug(f\"Input to rag_pipeline.invoke: {input_dict}\")\n",
    "    result = rag_pipeline.invoke(input_dict)\n",
    "    logging.debug(f\"Result from rag_pipeline.invoke: {result}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ee23f3-dbaf-41fe-bc80-630ddbf20190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage create agent\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the web for realtime and latest information.\n",
    "    for examples, news, stock market, weather updates etc.\n",
    "    \n",
    "    Args:\n",
    "    query: The search query\n",
    "    \"\"\"\n",
    "    \n",
    "    search = TavilySearchResults(\n",
    "        max_results=5,\n",
    "        search_depth=\"advanced\",\n",
    "        include_answer=True,\n",
    "        include_raw_content=True,\n",
    "    )\n",
    "    response = search.invoke(query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab5db4cf-4cb6-41cf-96a3-6a88dccd8dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 21:33:55.596 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-20 21:33:55.597 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-20 21:33:55.598 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-20 21:33:55.599 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-20 21:33:55.599 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "2025-01-20 21:33:55.600 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-20 21:33:55.859 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/namnh1/miniconda3/envs/rag/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-01-20 21:33:55.860 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-20 21:33:55.861 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-20 21:33:55.863 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-20 21:33:55.865 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-20 21:33:55.866 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-20 21:33:55.867 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-20 21:33:55.868 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "embedding_model_name = st.sidebar.text_input(\"Embedding Model\", value=\"nomic-embed-text\")\n",
    "model_url = st.sidebar.text_input(\"Model Base URL\", value=\"http://localhost:11434\")\n",
    "embeddings = OllamaEmbeddings(model=embedding_model_name, base_url=model_url)\n",
    "chat_model = ChatOllama(model=\"llama3\", base_url=model_url)\n",
    "\n",
    "tools =  [search]\n",
    "prompt_template = build_chatbot_prompt_template()\n",
    "agent = create_tool_calling_agent(llm=chat_model, tools=tools, prompt=prompt_template)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "885a6753-bf9c-4d8b-83fb-73e87ec973d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_694715/4058644294.py:21: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant_client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ScoredPoint(id='37b96fda-75b1-48c8-a7ee-609310778632', version=0, score=0.5997566, payload={'id': 0, 'text': 'User đăng nhập server\\nTài khoản deploy\\nTên đăng nhập: deployuser\\nMật khẩu: tmt@dxt2024\\nCác thư mục\\nCode trên máy tính cá nhân\\nD:\\\\coding\\\\dxtech\\nThư mục chứa SSL\\n/home/staging/deploy/dxtech_ssl\\nThư mục chứa code backend\\n/home/staging/deploy/dxtech-service/.. : các service code theo kiến trúc micro service\\n/home/staging/deploy/faceid-backend: customer monitoring chưa có micro service\\n/home/staging/deploy/ekyc-backend: ekyc chưa có micro service\\nĐường dẫn chứa các file service\\nsudo vi /etc/systemd/system/…\\nThư mục chứa code frontend\\n/home/staging/deploy/dxtech-frontend/..: các code frontend ởrepo mới\\n/home/staging/deploy//..: các code frontend ởrepo mới\\nThư mục chứa file config deploy frontend với nginx\\n/etc/nginx/sites-enabled\\nekyc: web ekyc\\nface-recognition: customer-monitoring\\ntest-faceid: customer-monitoring bản test'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='2ccf0f10-329e-4c71-b9aa-d86c44590a6f', version=0, score=0.5266129, payload={'id': 1, 'text': 'Ghi log\\n/opt/dxt-ekyc-logs/..\\nEureka server\\n/home/staging/deploy/dxtech-service/eureka-server\\nAPI gateway\\n/home/staging/deploy/dxtech-service/api-gateway\\nAuthentication (hệthống tài khoản)\\n/home/staging/deploy/dxtech-service/authen-service\\nCustomer monitoring\\n/home/staging/deploy/dxtech-service/\\nEkyc\\n/home/staging/deploy/dxtech-service/ekyc-backend\\nCách deploy Backend\\nBước 1: Build file JAR\\nBước 2: Chạy lại service\\nTên các file service:\\n-\\ndxtech-eureka-server.service (Quản lý eureka service)\\n-\\ndxtech-api-gateway.service (Gateway gom service cho người dùng gọi)\\n-\\ndxtech-customer-monitoring.service\\n-\\ndxtech-ekyc.service\\n-\\ndxtech-authentication.service (Hệthống cấp token sửdụng dịch vụ)'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='0765843a-7210-459e-b4c6-1ae570224fa1', version=0, score=0.4874844, payload={'id': 2, 'text': 'Pattern mẫu trong các file service\\n================================================================\\n[Unit]\\nDescription=Spring Boot Application\\nAfter=syslog.target\\nAfter=network.target\\n# Thay đổi\\n[Service]\\nUser= deployuser\\nExecStart=/usr/bin/java -jar /path/to/your/app.jar\\nSuccessExitStatus=143\\nRestart=on-failure\\nRestartSec=10\\nStandardOutput=syslog\\nStandardError=syslog\\nSyslogIdentifier=springboot-app\\n[Install]\\nWantedBy=multi-user.target\\n=================================================================\\nSau khi đã cấu hình xong, reload systemd đểáp dụng thay đổi và kích hoạt service:\\n-\\nsudo systemctl daemon-reload\\n-\\nsudo systemctl enable {tên service}\\nStart ứng dụng\\nsudo systemctl start {tên service}\\nRestart ứng dụng\\nsudo systemctl start {tên service}\\nCác lệnh hỗtrợ\\nKiểm tra trạng thái service\\n-\\nsudo systemctl status springboot-app.service'}, vector=None, shard_key=None, order_value=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_694715/2212122181.py:9: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(collection_name=collection_name, query_vector=query_vector, limit=3)\n"
     ]
    }
   ],
   "source": [
    "# Load file dataset\n",
    "file_pdf = \"load_file.pdf\"\n",
    "docs = load_and_split_pdf(file_pdf)\n",
    "\n",
    "# Test query vector database\n",
    "question = \"How to deploy backend\"\n",
    "qdrant_client, collection_name = create_vector_store_qdrant(docs, embeddings)\n",
    "query_vector = embeddings.embed_query(question)\n",
    "search_result = qdrant_client.search(collection_name=collection_name, query_vector=query_vector, limit=3)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5e01922-f3e9-432d-9610-8414145a3285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'User đăng nhập server\\nTài khoản deploy\\nTên đăng nhập: deployuser\\nMật khẩu: tmt@dxt2024\\nCác thư mục\\nCode trên máy tính cá nhân\\nD:\\\\coding\\\\dxtech\\nThư mục chứa SSL\\n/home/staging/deploy/dxtech_ssl\\nThư mục chứa code backend\\n/home/staging/deploy/dxtech-service/.. : các service code theo kiến trúc micro service\\n/home/staging/deploy/faceid-backend: customer monitoring chưa có micro service\\n/home/staging/deploy/ekyc-backend: ekyc chưa có micro service\\nĐường dẫn chứa các file service\\nsudo vi /etc/systemd/system/…\\nThư mục chứa code frontend\\n/home/staging/deploy/dxtech-frontend/..: các code frontend ởrepo mới\\n/home/staging/deploy//..: các code frontend ởrepo mới\\nThư mục chứa file config deploy frontend với nginx\\n/etc/nginx/sites-enabled\\nekyc: web ekyc\\nface-recognition: customer-monitoring\\ntest-faceid: customer-monitoring bản test\\n\\nGhi log\\n/opt/dxt-ekyc-logs/..\\nEureka server\\n/home/staging/deploy/dxtech-service/eureka-server\\nAPI gateway\\n/home/staging/deploy/dxtech-service/api-gateway\\nAuthentication (hệthống tài khoản)\\n/home/staging/deploy/dxtech-service/authen-service\\nCustomer monitoring\\n/home/staging/deploy/dxtech-service/\\nEkyc\\n/home/staging/deploy/dxtech-service/ekyc-backend\\nCách deploy Backend\\nBước 1: Build file JAR\\nBước 2: Chạy lại service\\nTên các file service:\\n-\\ndxtech-eureka-server.service (Quản lý eureka service)\\n-\\ndxtech-api-gateway.service (Gateway gom service cho người dùng gọi)\\n-\\ndxtech-customer-monitoring.service\\n-\\ndxtech-ekyc.service\\n-\\ndxtech-authentication.service (Hệthống cấp token sửdụng dịch vụ)\\n\\nPattern mẫu trong các file service\\n================================================================\\n[Unit]\\nDescription=Spring Boot Application\\nAfter=syslog.target\\nAfter=network.target\\n# Thay đổi\\n[Service]\\nUser= deployuser\\nExecStart=/usr/bin/java -jar /path/to/your/app.jar\\nSuccessExitStatus=143\\nRestart=on-failure\\nRestartSec=10\\nStandardOutput=syslog\\nStandardError=syslog\\nSyslogIdentifier=springboot-app\\n[Install]\\nWantedBy=multi-user.target\\n=================================================================\\nSau khi đã cấu hình xong, reload systemd đểáp dụng thay đổi và kích hoạt service:\\n-\\nsudo systemctl daemon-reload\\n-\\nsudo systemctl enable {tên service}\\nStart ứng dụng\\nsudo systemctl start {tên service}\\nRestart ứng dụng\\nsudo systemctl start {tên service}\\nCác lệnh hỗtrợ\\nKiểm tra trạng thái service\\n-\\nsudo systemctl status springboot-app.service'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format the retrieved documents\n",
    "retrieved_docs = [hit.payload[\"text\"] for hit in search_result]\n",
    "formatted_context = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "formatted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e7448e8-e31b-46d1-85e8-0160a9f6d956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'context', 'history', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'context', 'history', 'question'], input_types={}, partial_variables={}, template='\\n        You are a helpful assistant answering questions based on the provided context from uploaded documents. Your name is Detnam.\\n\\n        Conversation history (latest messages appear last):\\n        {history}\\n\\n        New question:\\n        {question}\\n\\n        Retrieved context:\\n        {context}\\n        \\n        Thought process:\\n        {agent_scratchpad}\\n\\n        Your response:\\n    '), additional_kwargs={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = build_chatbot_prompt_template()\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fae9a199-76aa-43fc-be3c-54820c4824a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_agent' from 'langchain.agents' (/home/namnh1/miniconda3/envs/rag/lib/python3.10/site-packages/langchain/agents/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_agent\n\u001b[1;32m      2\u001b[0m tools \u001b[38;5;241m=\u001b[39m  [search]\n\u001b[1;32m      3\u001b[0m agent \u001b[38;5;241m=\u001b[39m create_agent(llm\u001b[38;5;241m=\u001b[39mchat_model, tools\u001b[38;5;241m=\u001b[39mtools, prompt\u001b[38;5;241m=\u001b[39mprompt_template)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'create_agent' from 'langchain.agents' (/home/namnh1/miniconda3/envs/rag/lib/python3.10/site-packages/langchain/agents/__init__.py)"
     ]
    }
   ],
   "source": [
    "tools =  [search]\n",
    "agent = create_tool_calling_agent(llm=chat_model, tools=tools, prompt=prompt_template)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c17c4f8-d2e5-40bb-8f3a-9ea024dec5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22f605fd-6654-48f0-b45d-5b49a3dd3cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = (\n",
    "    RunnablePassthrough()\n",
    "    | {\n",
    "        \"agent_scratchpad\": lambda x: \"\",\n",
    "        \"context\": lambda x: formatted_context,  # Use a lambda to pass the context\n",
    "        \"history\": lambda x: history,            # Use a lambda to pass the history\n",
    "        \"question\": lambda x: question,          # Use a lambda to pass the question\n",
    "    }\n",
    "    | prompt_template\n",
    "    | agent_executor\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03e01719-a021-442c-8699-7c677e95be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {\n",
    "    \"agent_scratchpad\": \"\",\n",
    "    \"context\": formatted_context,\n",
    "    \"history\": history,\n",
    "    \"question\": question\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2225f04f-a2fb-4c47-9998-6aecdaf62e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_scratchpad': '',\n",
       " 'context': 'User đăng nhập server\\nTài khoản deploy\\nTên đăng nhập: deployuser\\nMật khẩu: tmt@dxt2024\\nCác thư mục\\nCode trên máy tính cá nhân\\nD:\\\\coding\\\\dxtech\\nThư mục chứa SSL\\n/home/staging/deploy/dxtech_ssl\\nThư mục chứa code backend\\n/home/staging/deploy/dxtech-service/.. : các service code theo kiến trúc micro service\\n/home/staging/deploy/faceid-backend: customer monitoring chưa có micro service\\n/home/staging/deploy/ekyc-backend: ekyc chưa có micro service\\nĐường dẫn chứa các file service\\nsudo vi /etc/systemd/system/…\\nThư mục chứa code frontend\\n/home/staging/deploy/dxtech-frontend/..: các code frontend ởrepo mới\\n/home/staging/deploy//..: các code frontend ởrepo mới\\nThư mục chứa file config deploy frontend với nginx\\n/etc/nginx/sites-enabled\\nekyc: web ekyc\\nface-recognition: customer-monitoring\\ntest-faceid: customer-monitoring bản test\\n\\nGhi log\\n/opt/dxt-ekyc-logs/..\\nEureka server\\n/home/staging/deploy/dxtech-service/eureka-server\\nAPI gateway\\n/home/staging/deploy/dxtech-service/api-gateway\\nAuthentication (hệthống tài khoản)\\n/home/staging/deploy/dxtech-service/authen-service\\nCustomer monitoring\\n/home/staging/deploy/dxtech-service/\\nEkyc\\n/home/staging/deploy/dxtech-service/ekyc-backend\\nCách deploy Backend\\nBước 1: Build file JAR\\nBước 2: Chạy lại service\\nTên các file service:\\n-\\ndxtech-eureka-server.service (Quản lý eureka service)\\n-\\ndxtech-api-gateway.service (Gateway gom service cho người dùng gọi)\\n-\\ndxtech-customer-monitoring.service\\n-\\ndxtech-ekyc.service\\n-\\ndxtech-authentication.service (Hệthống cấp token sửdụng dịch vụ)\\n\\nPattern mẫu trong các file service\\n================================================================\\n[Unit]\\nDescription=Spring Boot Application\\nAfter=syslog.target\\nAfter=network.target\\n# Thay đổi\\n[Service]\\nUser= deployuser\\nExecStart=/usr/bin/java -jar /path/to/your/app.jar\\nSuccessExitStatus=143\\nRestart=on-failure\\nRestartSec=10\\nStandardOutput=syslog\\nStandardError=syslog\\nSyslogIdentifier=springboot-app\\n[Install]\\nWantedBy=multi-user.target\\n=================================================================\\nSau khi đã cấu hình xong, reload systemd đểáp dụng thay đổi và kích hoạt service:\\n-\\nsudo systemctl daemon-reload\\n-\\nsudo systemctl enable {tên service}\\nStart ứng dụng\\nsudo systemctl start {tên service}\\nRestart ứng dụng\\nsudo systemctl start {tên service}\\nCác lệnh hỗtrợ\\nKiểm tra trạng thái service\\n-\\nsudo systemctl status springboot-app.service',\n",
       " 'history': '',\n",
       " 'question': 'How to deploy backend'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a48bb5-e1b9-4e05-87db-cffdf738c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag_pipeline.invoke(input_dict)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53ec9fd9-cecc-488e-a5b3-bca7f0b56468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namnh1/miniconda3/envs/rag/lib/python3.10/site-packages/langchain_community/utilities/duckduckgo_search.py:64: UserWarning: backend='api' is deprecated, using backend='auto'\n",
      "  ddgs_gen = ddgs.text(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nasdaq futures plunged on Monday to lead a stock rout on Wall Street as a Chinese startup rattled faith in US leadership and profitability in AI, taking a hammer to Nvidia and other Big Tech stocks. Find the latest stock market news from every corner of the globe at Reuters.com, your online source for breaking international market and finance news AP AUDIO: Stock market today: Wall Street rallies toward its best week since Trump's election The AP's Seth Sutel reports stocks keep moving higher. The Magnificent Seven have been under pressure recently because of criticism their stock prices may have shot too high after leading the market for so many years. Stock Market News And Analysis. The analysis you'll find in the Stock Market Today is based on over 130 years of market history and a detailed study of every top-performing stock since the 1880s. Stocks surged on Wednesday after the latest consumer price index report showed core inflation unexpectedly slowed in December.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "search.invoke(\"What is today's stock market news?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "352e654e-6ab7-4eb5-83ad-0adef23a32f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import streamlit as st\n",
    "import faiss\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain.schema.runnable.passthrough import RunnablePassthrough\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "\n",
    "# Add more agents\n",
    "from langchain.agents import create_tool_calling_agent, initialize_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f81ad46f-6c07-467c-b5e7-fed9d7315e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"How's it going? Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-01-27T14:42:03.69912351Z', 'done': True, 'done_reason': 'stop', 'total_duration': 339486338, 'load_duration': 39279293, 'prompt_eval_count': 11, 'prompt_eval_duration': 26000000, 'eval_count': 21, 'eval_duration': 272000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-610b4e42-fc18-46c6-be6f-73ecfc96628f-0', usage_metadata={'input_tokens': 11, 'output_tokens': 21, 'total_tokens': 32})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "llm = ChatOllama(model='llama3.1', base_url='http://localhost:11434')\n",
    "llm.invoke('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae1ed803-8d26-472a-8fc0-f04463ebbb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the web for realtime and latest information.\n",
    "    for examples, news, stock market, weather updates etc.\n",
    "    \n",
    "    Args:\n",
    "    query: The search query\n",
    "    \"\"\"\n",
    "    \n",
    "    search = DuckDuckGoSearchRun(\n",
    "    )\n",
    "    response = search.invoke(query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c964218-bdd0-4fef-85b6-3a41c79da1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92ff1608-3ed9-4ba8-a407-926b6366679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_pdf(file_path, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Load a PDF file and split it into chunks.\n",
    "    \"\"\"\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a23219b2-6f36-4019-8a1c-b166f6bcbd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e28f023e-2eb4-445f-871f-3a3627c6a553',\n",
       " '5ef07e30-caa2-4f5d-bb01-314d31363da4',\n",
       " '0b0dc12b-9324-4f13-b6ee-b77b0b2369f4',\n",
       " '742b3bc2-c034-4e65-b6f0-9125c854c42c']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OllamaEmbeddings(model='nomic-embed-text', base_url='http://localhost:11434')\n",
    "\n",
    "file_path = \"load_file.pdf\"\n",
    "\n",
    "doc_chunks = load_and_split_pdf(file_path)\n",
    "\n",
    "sample_vector = embeddings.embed_query(\"sample text\")\n",
    "index = faiss.IndexFlatL2(len(sample_vector))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "vector_store.add_documents(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cd09470-ea7e-453b-9720-6ac370ab63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type = 'similarity', \n",
    "                                      search_kwargs = {'k': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45a051f7-4aa7-4e30-9e11-c6f2efbc5b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namnh1/miniconda3/envs/rag/lib/python3.10/site-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tools = [search]\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "532df01d-e38f-47ad-bceb-b3445edcbcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chatbot_prompt_template():\n",
    "    prompt_text = \"\"\"\n",
    "        You are Detnam, a knowledgeable and helpful assistant. You answer questions using information from provided documents and context. Be clear, concise, and engaging in your responses.\n",
    "\n",
    "        Conversation History (most recent messages at the end):\n",
    "        {history}\n",
    "\n",
    "        User Question:\n",
    "        {question}\n",
    "\n",
    "        Relevant Context (retrieved from documents):\n",
    "        {context}\n",
    "        \n",
    "        Thought Process (reason through your answer):\n",
    "        {agent_scratchpad}\n",
    "\n",
    "        Please provide a well-structured, informative, and friendly response:\n",
    "    \"\"\"\n",
    "    return ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "prompt = build_chatbot_prompt_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a28578a2-7938-4f91-8586-87233d8893a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ab589c1-2e96-4cb9-a883-fb6821db87fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search` with `{'query': 'deploying a backend server'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namnh1/miniconda3/envs/rag/lib/python3.10/site-packages/langchain_community/utilities/duckduckgo_search.py:64: UserWarning: backend='api' is deprecated, using backend='auto'\n",
      "  ddgs_gen = ddgs.text(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mHow to deploy node express app on vercel free step by step guide. Introduction. Deploying your Node.js backend project can seem daunting, but with platforms like Vercel, the process is streamlined ... To deploy a Node.js backend to Vercel, you can follow the simple approach that involves setting up your Node.js project, configuring necessary files, and using the Vercel CLI. Vercel is known for its ease of use and automatic optimizations, making it an excellent choice for deploying full-stack appl In this tutorial, you will learn how to deploy or host your backend project using Vercel serverless function via GitHub. Before we dive deep into building and configuration, let's understand the concept of serverless function in Vercel. Transitioning from a traditional Express.js server to build-in Vercel Functions. Taking this step means you can stop configuring your own server from scratch and let Vercel manage that for you. Transitioning to Next.js if you are migrating from an Express.js Server and React SPA. Taking this step would reduce the overhead and maintenance of ... Trigger manual initial deployment. 4. Configure Environment Variables. Sensitive keys should be configured under Settings > Config Vars rather than code. And that's it! Now code changes trigger auto re-deployment upon GitHub pushes. Key Takeaways. We covered end-to-end implementation details spanning: Express + Node - fast, scalable server ...\u001b[0m\u001b[32;1m\u001b[1;3mIt seems like the tool call response was able to provide a detailed step-by-step guide on how to deploy a backend server using Vercel.\n",
      "\n",
      "To summarize:\n",
      "\n",
      "1. Set up your Node.js project.\n",
      "2. Configure necessary files.\n",
      "3. Use the Vercel CLI.\n",
      "4. Trigger manual initial deployment.\n",
      "5. Configure Environment Variables.\n",
      "\n",
      "Additionally, it covers transitioning from traditional Express.js servers to build-in Vercel Functions and migrating from Express.js Server and React SPA to Next.js.\n",
      "\n",
      "Please let me know if you have any further questions or concerns!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"how to deploy backend?\"\n",
    "response = agent_executor.invoke({'input': question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45612ca1-4253-4162-9bed-5ac5e2530e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like the tool call response was able to provide a detailed step-by-step guide on how to deploy a backend server using Vercel.\n",
      "\n",
      "To summarize:\n",
      "\n",
      "1. Set up your Node.js project.\n",
      "2. Configure necessary files.\n",
      "3. Use the Vercel CLI.\n",
      "4. Trigger manual initial deployment.\n",
      "5. Configure Environment Variables.\n",
      "\n",
      "Additionally, it covers transitioning from traditional Express.js servers to build-in Vercel Functions and migrating from Express.js Server and React SPA to Next.js.\n",
      "\n",
      "Please let me know if you have any further questions or concerns!\n"
     ]
    }
   ],
   "source": [
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb775794-753b-48db-ac50-6d8cb193a68b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
