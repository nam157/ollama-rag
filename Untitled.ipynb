{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0850156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "file_path = \"/home/namnh1/rag-llm-chatbot/BC AI/bc.xlsx\"\n",
    "df = UnstructuredExcelLoader(file_path=file_path)\n",
    "content = df.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f8b0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/home/namnh1/rag-llm-chatbot/BC AI/bc.xlsx'}, page_content='TỔNG HỢP CÔNG NỢ OEM TUẦN 2025-03-08 00:00:00 Tuần 1 tháng 3- từ ngày 01/3 đến 7/3/2025 TT Tên đại lý Nợ ĐK Có ĐK PS Nợ Thu tiền Trả lại, CK Nợ CK Có CK KẾ HOẠCH Đặc điểm Ngày PS Đặt hàng Tồn kho Quá hạn tt Ghi chú Ghi chú Ngày đưa xác nhận CN Ngày nhận xác nhận CN HĐNT\\\\nNỘI BỘ HĐNT\\\\nTHUẾ CAM KẾT 2024 1 Nguyễn Thị Lan Thu 600000000 170000000 4000000000 341850000 0 3580000000 -8150000 0 1 OEMDUCTOAN\\\\nPhương: Đức Toàn\\\\nGold mark 100000000 41850000 -58150000 Bản in 5tr\\\\nĐặt 30%\\\\nTT : 1 tháng 2024-04-06 00:00:00 27/6 2024 đã ký 2023,2024 đã ký HB Đã ký 2 OEMHUNGT\\\\nTrường: Hưng Thịnh\\\\nMalayone 500000000 Đặt 30%\\\\nTT ngay 2025-04-03 00:00:00 - 3 OEMNGHIN\\\\nA Nghinh\\\\nSunshise 0 70000000 500000000 430000000 Đặt 30%\\\\nTT ngay 2025-04-03 00:00:00 4/5\\\\n4/6 13/6 2024 đã ký Không Không 4 OEMHAVITECH\\\\nHavitech 0 500000000 500000000 Đặt 30%\\\\nTT ngay 2025-04-03 00:00:00 - 5 OEMKAISE\\\\nNgọc: Smarttech\\\\nKaiser 100000000 500000000 600000000 Đặt 30%\\\\nTT ngay 2024-08-12 00:00:00 500000000 4/5\\\\n4/6 2024-09-07 00:00:00 2024 đã ký Đã ký 6 OEMNAMT\\\\nNam Thắng\\\\nTH Ffloor, Meta 100000000 500000000 600000000 Đặt 5tr bản in\\\\nko cọc, TT ngay\\\\nCN cùng thời điểm 3 tỷ: TT trước 27/2-1/3 -2100000000 4/5\\\\n4/6 11/5\\\\n14/6 2024 đã ký 2023,2024 đã ký HB\\\\n2023 đã ký VIN Đã ký SPCNAMT\\\\nNam Thắng\\\\nTH Floor 4, 6 100000000 500000000 300000000 300000000 27/2-1/3 4/5\\\\n4/6 2024-11-05 00:00:00 7 OEMTHANH\\\\nThành: An Trạch\\\\nEtihad 100000000 500000000 600000000 Đặt 20%\\\\nTT ngay 24/2 450000000 2024-04-06 00:00:00 13/6 2024 đã ký Không Đã ký 8 OEMTT\\\\nTiến: Lê Văn Lương\\\\nMazdani 100000000 500000000 600000000 - Hỗ trợ Đặt hàng ko cần Đặt cọc\\\\n- TT Dần sau khi lấy hàng \\\\n(Tết ÂL TT Hết)\\\\n- Công nợ cùng 1 thời điểm ko vượt quá 300 triệu 23/8-28/11/24 9 OEMPENTA\\\\nCông ty Penta C&T\\\\n- DA Phào nhựa 0 50000000 50000000 Đặt 30%\\\\nTT ngay 10 OEMDATTHANH\\\\nCông ty Đạt Thành 0 50000000 -50000000\\n\\nTra cứu thông tin theo tên đại lý: ví dụ Nam Thắng tuần này thanh toán bao nhiêu, tổng công nợ bao nhiêu, phát sinh từ ngày nào?')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8efbe0d8-8942-4ecd-bc8a-5a47e4b2f708",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[1;32m     82\u001b[0m directory_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/namnh1/rag-llm-chatbot/BC AI\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 83\u001b[0m doc_processors \u001b[38;5;241m=\u001b[39m \u001b[43mDocumentProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mDocumentProcessor.__init__\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_overlap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m OllamaEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnomic-embed-text\u001b[39m\u001b[38;5;124m'\u001b[39m, base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:11434\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 80\u001b[0m, in \u001b[0;36mDocumentProcessor.process_documents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     79\u001b[0m splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_overlap)\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_rag/lib/python3.10/site-packages/langchain_text_splitters/base.py:94\u001b[0m, in \u001b[0;36mTextSplitter.split_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     92\u001b[0m texts, metadatas \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m---> 94\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m)\n\u001b[1;32m     95\u001b[0m     metadatas\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_documents(texts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.chunk_size = 5000\n",
    "        self.chunk_overlap = 200\n",
    "        self.embeddings = OllamaEmbeddings(model='nomic-embed-text', base_url=\"http://localhost:11434\")\n",
    "        self.docs = self.process_documents()\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def preprocess_excel(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Preprocess Excel-specific data:\n",
    "        - Fill empty cells with a default value.\n",
    "        - Remove completely empty columns.\n",
    "        - Normalize data types to strings, handling all datetime types correctly.\n",
    "        \"\"\"\n",
    "        df = df.dropna(axis=1, how='all')  # Remove completely empty columns\n",
    "        df = df.fillna('')  # Fill empty values with an empty string\n",
    "\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: (\n",
    "                    x.strftime('%Y-%m-%d %H:%M:%S') if isinstance(x, (pd.Timestamp, datetime.datetime))\n",
    "                    else str(x).strip() if isinstance(x, str) and x != '' \n",
    "                    else str(x) if x != '' \n",
    "                    else ''\n",
    "                )\n",
    "            )\n",
    "\n",
    "        df.columns = [re.sub(r'\\s+', '_', col.strip()) for col in df.columns]  # Normalize column names\n",
    "        return df\n",
    "\n",
    "    def process_documents(self):\n",
    "        # converter = DocumentConverter()\n",
    "        documents = []\n",
    "\n",
    "        if os.path.isfile(self.file_path):\n",
    "            file_paths = [self.file_path]\n",
    "        elif os.path.isdir(self.file_path):\n",
    "            file_paths = [\n",
    "                os.path.join(self.file_path, f) for f in os.listdir(self.file_path)\n",
    "                if f.lower().endswith(('.pdf', '.docx', '.doc', '.xls', '.xlsx'))\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid path: {self.file_path}\")\n",
    "\n",
    "        for path in file_paths:\n",
    "            try:\n",
    "                # if path.lower().endswith(('.xls', '.xlsx')):\n",
    "                #     df = pd.read_excel(path)\n",
    "                #     df = self.preprocess_excel(df)\n",
    "                #     text_content = df.to_csv(index=False)\n",
    "                # else:\n",
    "                #     result = converter.convert(path)\n",
    "                #     text_content = result.document.export_to_markdown()\n",
    "\n",
    "                # preprocessed_text = self.preprocess_text(text_content)\n",
    "                # from langchain.docstore.document import Document\n",
    "                # doc = Document(page_content=preprocessed_text, metadata={\"source\": path})\n",
    "                df = UnstructuredExcelLoader(file_path=path)\n",
    "                doc = df.load()\n",
    "                documents.append(doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {path}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "        return splitter.split_documents(documents)\n",
    "\n",
    "directory_path = \"/home/namnh1/rag-llm-chatbot/BC AI\"\n",
    "doc_processors = DocumentProcessor(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b34670a-031f-4c89-a38c-9baf20a5c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import CrossEncoder\n",
    "from qdrant_client.http.models import HnswConfigDiff, VectorParams, Distance, PointStruct\n",
    "from qdrant_client import QdrantClient\n",
    "from typing import List\n",
    "import uuid\n",
    "\n",
    "class QdrantDB:\n",
    "    def __init__(self, host=\"10.100.140.54\", port=6333, embeddings=None):\n",
    "        self.client = QdrantClient(host=host, port=port)\n",
    "        self.embeddings = embeddings\n",
    "        self.reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "        self.bm25 = None  # Will store the BM25 index after indexing\n",
    "\n",
    "    def create_collection(self, collection_name):\n",
    "        existing_collections = [col.name for col in self.client.get_collections().collections]\n",
    "        if collection_name not in existing_collections:\n",
    "            self.client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=len(self.embeddings.embed_query(\"wellcome to eBot (DXTech)\")),\n",
    "                    distance=Distance.DOT\n",
    "                ),\n",
    "                hnsw_config=HnswConfigDiff(\n",
    "                    m=16,\n",
    "                    ef_construct=100,\n",
    "                    full_scan_threshold=10000\n",
    "                )\n",
    "            )\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def insert_documents(self, collection_name: str, doc_chunks: List, embeddings):\n",
    "        vectors = [embeddings.embed_query(chunk.page_content) for chunk in doc_chunks]\n",
    "        payloads = [{\"id\": str(uuid.uuid4()), \"text\": chunk.page_content} for chunk in doc_chunks]\n",
    "        points = [PointStruct(id=payload[\"id\"], vector=vectors[i], payload=payload) for i, payload in enumerate(payloads)]\n",
    "        \n",
    "        self.client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "        # Precompute BM25 index during insertion\n",
    "        texts = [chunk.page_content for chunk in doc_chunks]\n",
    "        tokenized_corpus = [text.split() for text in texts]\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    def rerank_documents(self, question: str, documents: List[str], top_k: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Rerank a list of documents based on relevance to the question using CrossEncoder.\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        rerank_inputs = [[question, doc] for doc in documents]\n",
    "        scores = self.reranker.predict(rerank_inputs, batch_size=32)\n",
    "        sorted_pairs = sorted(zip(scores, documents), reverse=True)\n",
    "        return [doc for _, doc in sorted_pairs][:top_k]\n",
    "    \n",
    "    def search_database_fusion_bm250(self, collection_name, question, embeddings, limit=5):\n",
    "        if self.bm25 is None:\n",
    "            # Fetch all documents from Qdrant if BM25 index isn’t precomputed\n",
    "            all_docs = self.client.scroll(\n",
    "                collection_name=collection_name,\n",
    "                limit=10000,  # Adjust based on your collection size\n",
    "                with_payload=True\n",
    "            )[0]\n",
    "            texts = [doc.payload['text'] for doc in all_docs if 'text' in doc.payload]\n",
    "            tokenized_corpus = [text.split() for text in texts]\n",
    "            self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "        # BM25 Search\n",
    "        bm25_scores = self.bm25.get_scores(question.split())\n",
    "        top_bm25_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:limit]\n",
    "        all_docs = self.client.scroll(collection_name=collection_name, limit=10000)[0]  # Fetch all docs\n",
    "        top_bm25_results = [all_docs[i].payload['text'] for i in top_bm25_indices if i < len(all_docs)]\n",
    "\n",
    "        # Vector Search\n",
    "        query_vector = embeddings.embed_query(question)\n",
    "        search_result = self.client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=limit,\n",
    "            score_threshold=0.6\n",
    "        )\n",
    "\n",
    "        # Fusion Search\n",
    "        fusion_results = top_bm25_results + [hit.payload['text'] for hit in search_result if 'text' in hit.payload]\n",
    "        fusion_results = list(set(fusion_results))  # Remove duplicates\n",
    "\n",
    "        # Rerank combined results\n",
    "        reranked_docs = self.rerank_documents(question, fusion_results, top_k=limit)\n",
    "        return \"\\n\\n\".join(reranked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8398dafa-9a60-452d-91e9-2f64e7a77b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Bạn là một trợ lý thông minh, chuyên về phân tích dữ liệu và hỗ trợ truy xuất thông tin (RAG). Nhiệm vụ của bạn là sử dụng dữ liệu và lịch sử hội thoại để đưa ra những phân tích chính xác, tóm tắt thông tin và giải đáp các thắc mắc của người dùng một cách rõ ràng, chi tiết và có căn cứ.\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Conversation History]\n",
    "{history}\n",
    "\n",
    "[User Question]\n",
    "{question}\n",
    "\n",
    "Your Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79e24bd4-c9bd-4e3e-ad5a-fcff268b9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable.passthrough import RunnablePassthrough\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "class PipelineBot:\n",
    "    def __init__(self,qdrant_client, collection_name, embeddings, document_processor, model_name = \"deepseek-r1:14b\"):\n",
    "        self.llm = ChatOllama(model=model_name, temperature=0)\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "        self.embeddings = embeddings\n",
    "        self.document_processor = document_processor\n",
    "\n",
    "    \n",
    "    def build_chain(self, qdrant_client, collection_name, embeddings, question, history):\n",
    "        # retrieved_context = qdrant_client.search_database_fusion_bm250(collection_name, question, embeddings, self.document_processor.docs)\n",
    "        retrieved_context = qdrant_client.search_database_fusion_bm250(\n",
    "            collection_name=collection_name,\n",
    "            question=question,\n",
    "            embeddings=embeddings,\n",
    "            limit=5  # Explicitly pass limit if you want to customize it\n",
    "        )\n",
    "        pipeline = (\n",
    "            RunnablePassthrough()\n",
    "            | {\"history\": lambda x: history, \"question\": lambda x: question, \"context\": lambda x: retrieved_context}\n",
    "            | ChatPromptTemplate.from_template(prompt_template)\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        result = pipeline.invoke({})\n",
    "        return result\n",
    "    def __call__(self, question, history):\n",
    "        return self.build_chain(self.qdrant_client, self.collection_name, self.embeddings, question, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5176b9a-b3ad-450f-a6a9-9544573df359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lỗi khi xử lý file /home/namnh1/rag-llm-chatbot/BC AI/BC Công nợ OEM tuần.xlsx: 'datetime.datetime' object has no attribute 'strip'\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"/home/namnh1/rag-llm-chatbot/BC AI\"\n",
    "doc_processors = DocumentProcessor(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95f6f6a4-765b-4fe1-a5db-3458e42ee924",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_db = QdrantDB(embeddings=doc_processors.embeddings) if doc_processors else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c72156e-4fc3-4f75-85f4-9962fe67af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"hobiwood\"\n",
    "qdrant_db.create_collection(collection_name)\n",
    "qdrant_db.insert_documents(collection_name, doc_chunks = doc_processors.docs, embeddings=doc_processors.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c33db11-df2a-40cf-834d-9cb449a09c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = PipelineBot(\n",
    "        qdrant_client=qdrant_db, \n",
    "        collection_name=collection_name, \n",
    "        embeddings=doc_processors.embeddings, \n",
    "        document_processor=doc_processors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8da6047d-4433-4478-80b4-fec9b2cb0ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1782533/1001733337.py:77: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = self.client.search(\n"
     ]
    }
   ],
   "source": [
    "question = \"Công ty cổ phần S- Decoro có giá trị hợp đồng là bao nhiêu \"\n",
    "response = chatbot(question, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6893dfa-f63c-436e-a71e-981687feba74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nĐầu tiên, tôi xem xét dữ liệu được cung cấp để tìm kiếm thông tin về giá trị hợp đồng của Công ty cổ phần S-Decoro. Tuy nhiên, trong dữ liệu này không đề cập đến bất kỳ chi tiết nào liên quan đến công ty này. Tất cả các thông tin đều xoay quanh Công ty Cổ Phần Hải Phòng Invest và một số OEM khác như OEM Ductoan và OEM Hungt.\\n\\nDo đó, tôi không thể xác định được giá trị hợp đồng của S-Decoro từ dữ liệu hiện tại. Tôi khuyên người dùng nên liên hệ trực tiếp với công ty hoặc tìm kiếm thông tin từ các nguồn chính thức để có được thông tin chính xác.\\n</think>\\n\\nCông ty cổ phần S-Decoro không có thông tin trong dữ liệu được cung cấp.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeafd1a-66ae-41b4-bf72-671401cdaf66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
